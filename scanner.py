"""
SentinelWeb - Engine de Monitoramento (Scanner)
===============================================
Este m√≥dulo cont√©m toda a l√≥gica de verifica√ß√£o de seguran√ßa:
- Check de Uptime (HTTP Status)
- Verifica√ß√£o de Certificado SSL
- Scan de Portas Cr√≠ticas

IMPORTANTE: Todas as fun√ß√µes usam timeouts curtos (5s) para n√£o travar
a fila de processamento se um site estiver offline ou lento.
"""

import socket
import ssl
import time
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass
import httpx
from OpenSSL import crypto
import asyncio
import os
import requests
import whois
import re
import dns.resolver  # Para verifica√ß√£o de blacklist (RBL)


# Timeout padr√£o para todas as opera√ß√µes de rede (em segundos)
DEFAULT_TIMEOUT = 5

# Token do Bot do Telegram (configurado via vari√°vel de ambiente)
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "")

# Portas cr√≠ticas que devem ser monitoradas
# Essas portas abertas podem representar riscos de seguran√ßa
CRITICAL_PORTS = {
    21: "FTP",      # File Transfer Protocol - transfer√™ncia de arquivos sem criptografia
    22: "SSH",      # Secure Shell - acesso remoto (pode ser leg√≠timo, mas monitoramos)
    23: "Telnet",   # Protocolo antigo sem criptografia - alto risco
    3306: "MySQL",  # Banco de dados MySQL exposto - risco cr√≠tico
    5432: "PostgreSQL",  # Banco de dados PostgreSQL exposto - risco cr√≠tico
    27017: "MongoDB",    # Banco de dados MongoDB exposto - risco cr√≠tico
    6379: "Redis",       # Redis exposto - risco cr√≠tico
}


@dataclass
class ScanResult:
    """
    Classe que encapsula todos os resultados de uma verifica√ß√£o.
    Facilita o transporte de dados entre fun√ß√µes.
    """
    # Uptime Check
    is_online: bool = False
    http_status_code: Optional[int] = None
    latency_ms: Optional[float] = None
    
    # SSL Check
    ssl_valid: Optional[bool] = None
    ssl_days_remaining: Optional[int] = None
    ssl_issuer: Optional[str] = None
    ssl_error: Optional[str] = None
    
    # Port Scan
    open_ports: List[int] = None
    
    # Erro geral
    error_message: Optional[str] = None
    
    def __post_init__(self):
        if self.open_ports is None:
            self.open_ports = []
    
    def to_dict(self) -> Dict[str, Any]:
        """Converte para dicion√°rio para serializa√ß√£o"""
        return {
            "is_online": self.is_online,
            "http_status_code": self.http_status_code,
            "latency_ms": self.latency_ms,
            "ssl_valid": self.ssl_valid,
            "ssl_days_remaining": self.ssl_days_remaining,
            "ssl_issuer": self.ssl_issuer,
            "ssl_error": self.ssl_error,
            "open_ports": self.open_ports,
            "error_message": self.error_message,
        }


def send_telegram_alert(message: str, chat_id: str) -> bool:
    """
    Envia um alerta via Telegram Bot API.
    
    Args:
        message: Mensagem a ser enviada
        chat_id: ID do chat do Telegram do usu√°rio
    
    Returns:
        True se enviado com sucesso, False caso contr√°rio
    
    Security Notes:
        - Usa HTTPS por padr√£o
        - Token deve estar em vari√°vel de ambiente
        - Timeout de 10 segundos para evitar travamento
    """
    if not TELEGRAM_BOT_TOKEN:
        print("‚ö†Ô∏è  TELEGRAM_BOT_TOKEN n√£o configurado. Alerta n√£o enviado.")
        return False
    
    if not chat_id:
        print("‚ö†Ô∏è  chat_id n√£o fornecido. Alerta n√£o enviado.")
        return False
    
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    
    payload = {
        "chat_id": chat_id,
        "text": message,
        "parse_mode": "HTML"  # Permite formata√ß√£o HTML na mensagem
    }
    
    try:
        response = requests.post(url, json=payload, timeout=10)
        
        if response.status_code == 200:
            print(f"‚úÖ Alerta Telegram enviado para chat_id {chat_id}")
            return True
        else:
            print(f"‚ùå Erro ao enviar alerta Telegram: {response.status_code} - {response.text}")
            return False
            
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Exce√ß√£o ao enviar alerta Telegram: {e}")
        return False


def check_domain_expiration(domain: str) -> Optional[datetime]:
    """
    Verifica a data de expira√ß√£o do dom√≠nio usando Whois.
    
    Args:
        domain: O dom√≠nio a verificar (sem protocolo, ex: "google.com")
    
    Returns:
        datetime: Data de expira√ß√£o do dom√≠nio, ou None se n√£o conseguir obter
    
    Notes:
        - Remove protocolo (http://, https://) e paths da URL
        - O campo expiration_date pode retornar lista ou string
        - Em caso de erro (dom√≠nio inv√°lido, timeout, etc), retorna None
        - N√£o quebra a execu√ß√£o do monitoramento se falhar
    
    Examples:
        >>> check_domain_expiration("google.com")
        datetime(2024, 9, 14, 4, 0)
        
        >>> check_domain_expiration("invalido-xyz-123.com")
        None
    """
    try:
        # Limpa o dom√≠nio: remove protocolo, www, paths e querystrings
        clean_domain = domain.lower()
        clean_domain = re.sub(r'^https?://', '', clean_domain)
        clean_domain = re.sub(r'^www\.', '', clean_domain)
        clean_domain = clean_domain.split('/')[0]  # Removes path
        clean_domain = clean_domain.split('?')[0]  # Removes query string
        
        if not clean_domain:
            print(f"‚ö†Ô∏è  Dom√≠nio vazio ap√≥s limpeza: {domain}")
            return None
        
        print(f"üîç Consultando Whois para: {clean_domain}")
        
        # Faz a consulta Whois
        w = whois.whois(clean_domain)
        
        if not w:
            print(f"‚ö†Ô∏è  Whois retornou vazio para: {clean_domain}")
            return None
        
        expiration = w.expiration_date
        
        # O campo expiration_date pode ser:
        # - None (n√£o encontrado)
        # - datetime (√∫nico)
        # - Lista de datetime (m√∫ltiplas datas)
        # - String (algumas bibliotecas retornam string)
        
        if expiration is None:
            print(f"‚ö†Ô∏è  Data de expira√ß√£o n√£o encontrada para: {clean_domain}")
            return None
        
        # Se for lista, pega a primeira data (geralmente a mais pr√≥xima)
        if isinstance(expiration, list):
            if len(expiration) > 0:
                expiration = expiration[0]
            else:
                print(f"‚ö†Ô∏è  Lista de expira√ß√£o vazia para: {clean_domain}")
                return None
        
        # Se for string, tenta converter para datetime
        if isinstance(expiration, str):
            from dateutil import parser
            expiration = parser.parse(expiration)
        
        # Verifica se √© datetime v√°lido
        if isinstance(expiration, datetime):
            print(f"‚úÖ Data de expira√ß√£o encontrada: {expiration.strftime('%Y-%m-%d')} para {clean_domain}")
            return expiration
        else:
            print(f"‚ö†Ô∏è  Tipo de data inv√°lido para {clean_domain}: {type(expiration)}")
            return None
            
    except whois.parser.PywhoisError as e:
        print(f"‚ùå Erro Whois para {domain}: {e}")
        return None
    except Exception as e:
        print(f"‚ùå Erro inesperado ao consultar Whois para {domain}: {e}")
        return None


def check_blacklist(domain: str, timeout: float = 2.0) -> Tuple[bool, List[str]]:
    """
    Verifica se o dom√≠nio est√° listado em blacklists (RBL - Real-time Blackhole List).
    
    Como funciona:
    1. Resolve o IP do dom√≠nio
    2. Inverte o IP (ex: 1.2.3.4 vira 4.3.2.1)
    3. Consulta o IP invertido em listas RBL conhecidas
    4. Se houver resposta DNS, o IP est√° listado
    
    Args:
        domain: Dom√≠nio a verificar (sem protocolo, ex: "google.com")
        timeout: Timeout para cada consulta DNS (padr√£o: 2 segundos)
    
    Returns:
        Tuple[bool, List[str]]: (is_blacklisted, lista_de_blacklists_onde_foi_encontrado)
    
    Security Notes:
        - Usa timeout curto (2s) para n√£o travar o worker
        - Verifica m√∫ltiplas blacklists populares
        - Remove protocolo e paths da URL
        - Retorna lista vazia se n√£o estiver em nenhuma blacklist
    
    Example:
        >>> check_blacklist("malicious-site.com")
        (True, ["zen.spamhaus.org", "bl.spamcop.net"])
        
        >>> check_blacklist("google.com")
        (False, [])
    """
    # Lista de RBLs populares para verificar
    RBL_PROVIDERS = [
        "zen.spamhaus.org",      # Spamhaus (mais popular)
        "bl.spamcop.net",        # SpamCop
        "b.barracudacentral.org", # Barracuda
        "dnsbl.sorbs.net",       # SORBS
        "cbl.abuseat.org",       # Composite Blocking List
    ]
    
    # Remove protocolo e paths da URL
    clean_domain = re.sub(r'^https?://', '', domain)
    clean_domain = re.sub(r'^www\.', '', clean_domain)
    clean_domain = clean_domain.split('/')[0]
    
    blacklisted_in = []
    
    try:
        # 1. Resolve o IP do dom√≠nio
        ip_address = socket.gethostbyname(clean_domain)
        print(f"üîç IP resolvido para {clean_domain}: {ip_address}")
        
        # 2. Inverte o IP (1.2.3.4 -> 4.3.2.1)
        octets = ip_address.split('.')
        reversed_ip = '.'.join(reversed(octets))
        
        # 3. Verifica em cada RBL
        resolver = dns.resolver.Resolver()
        resolver.timeout = timeout
        resolver.lifetime = timeout
        
        for rbl in RBL_PROVIDERS:
            query = f"{reversed_ip}.{rbl}"
            
            try:
                # Se a consulta retornar resultado, o IP est√° listado
                answers = resolver.resolve(query, 'A')
                
                if answers:
                    print(f"‚ö†Ô∏è  BLACKLIST DETECTADA: {clean_domain} ({ip_address}) listado em {rbl}")
                    blacklisted_in.append(rbl)
                    
            except dns.resolver.NXDOMAIN:
                # NXDOMAIN significa que N√ÉO est√° listado (resposta esperada)
                pass
            except dns.resolver.NoAnswer:
                # Sem resposta, tamb√©m significa que n√£o est√° listado
                pass
            except dns.resolver.Timeout:
                # Timeout na consulta, ignora este RBL
                print(f"‚è±Ô∏è  Timeout ao consultar {rbl} para {clean_domain}")
                pass
            except Exception as e:
                # Outros erros, ignora este RBL
                print(f"‚ö†Ô∏è  Erro ao consultar {rbl}: {e}")
                pass
        
        # Resultado final
        is_blacklisted = len(blacklisted_in) > 0
        
        if is_blacklisted:
            print(f"üö® ALERTA: {clean_domain} est√° em {len(blacklisted_in)} blacklist(s): {', '.join(blacklisted_in)}")
        else:
            print(f"‚úÖ {clean_domain} n√£o est√° em nenhuma blacklist verificada")
        
        return is_blacklisted, blacklisted_in
        
    except socket.gaierror:
        # N√£o conseguiu resolver o dom√≠nio (pode estar offline ou inv√°lido)
        print(f"‚ö†Ô∏è  N√£o foi poss√≠vel resolver IP para {clean_domain}")
        return False, []
    except Exception as e:
        print(f"‚ùå Erro inesperado ao verificar blacklist para {domain}: {e}")
        return False, []


def check_seo_health(domain: str, timeout: int = DEFAULT_TIMEOUT) -> Dict[str, Any]:
    """
    Verifica se o site est√° bloqueando motores de busca (Google/Bing).
    
    VERIFICA√á√ïES CR√çTICAS:
    1. Meta tag noindex no HTML
    2. HTTP Header X-Robots-Tag
    3. Robots.txt bloqueando tudo (Disallow: /)
    
    Args:
        domain: Dom√≠nio a verificar (sem protocolo, ex: "example.com")
        timeout: Timeout para cada requisi√ß√£o (padr√£o: 5 segundos)
    
    Returns:
        Dict com:
        - indexable: bool (True = OK, False = BLOQUEADO)
        - issues: List[str] com problemas encontrados
        - robots_txt_content: str com conte√∫do do robots.txt (se existir)
        - error: str ou None se houver erro
    
    Example:
        >>> check_seo_health("example.com")
        {
            'indexable': False,
            'issues': ['Meta tag noindex encontrada', 'Robots.txt bloqueia o site'],
            'robots_txt_content': 'User-agent: *\\nDisallow: /',
            'error': None
        }
    """
    url = f"https://{domain}"
    result = {
        'indexable': True,
        'issues': [],
        'robots_txt_content': None,
        'error': None
    }
    
    try:
        # ============================================
        # CHECK 1: Meta Tag Noindex no HTML
        # ============================================
        print(f"üîç Verificando meta tags SEO em {domain}...")
        
        try:
            response = httpx.get(
                url,
                timeout=timeout,
                follow_redirects=True,
                headers={
                    'User-Agent': 'Mozilla/5.0 (compatible; SentinelWeb SEO Checker/1.0)'
                }
            )
            
            html_content = response.text.lower()
            
            # Regex para encontrar meta tags robots/googlebot com noindex
            # Exemplos que devem pegar:
            # <meta name="robots" content="noindex">
            # <meta name="robots" content="noindex, nofollow">
            # <meta name="googlebot" content="noindex">
            # <meta name="ROBOTS" content="NOINDEX, NOFOLLOW">
            
            meta_robots_pattern = r'<meta\s+name=["\']?(robots|googlebot)["\']?\s+content=["\']?[^"\']*noindex[^"\']*["\']?'
            
            if re.search(meta_robots_pattern, html_content, re.IGNORECASE):
                result['indexable'] = False
                result['issues'].append('üö® Meta tag noindex encontrada no HTML')
                print(f"  ‚ùå Meta tag noindex detectada!")
            else:
                print(f"  ‚úÖ Nenhuma meta tag noindex encontrada")
            
            # ============================================
            # CHECK 2: HTTP Header X-Robots-Tag
            # ============================================
            print(f"üîç Verificando HTTP headers...")
            
            x_robots_tag = response.headers.get('X-Robots-Tag', '').lower()
            
            if 'noindex' in x_robots_tag:
                result['indexable'] = False
                result['issues'].append(f'üö® HTTP Header X-Robots-Tag: {x_robots_tag}')
                print(f"  ‚ùå X-Robots-Tag com noindex detectado: {x_robots_tag}")
            else:
                print(f"  ‚úÖ Header X-Robots-Tag OK")
        
        except httpx.HTTPError as e:
            print(f"  ‚ö†Ô∏è Erro ao buscar HTML: {e}")
            result['error'] = f"Erro HTTP: {str(e)}"
        
        # ============================================
        # CHECK 3: Robots.txt Global Disallow
        # ============================================
        print(f"üîç Verificando robots.txt...")
        
        try:
            robots_url = f"{url}/robots.txt"
            robots_response = httpx.get(
                robots_url,
                timeout=timeout,
                follow_redirects=False,
                headers={
                    'User-Agent': 'Mozilla/5.0 (compatible; SentinelWeb SEO Checker/1.0)'
                }
            )
            
            if robots_response.status_code == 200:
                robots_content = robots_response.text
                result['robots_txt_content'] = robots_content
                
                print(f"  ‚úÖ Robots.txt encontrado ({len(robots_content)} bytes)")
                
                # Verifica bloqueio global (User-agent: * + Disallow: /)
                # Regex para detectar:
                # User-agent: *
                # Disallow: /
                
                # Normaliza o conte√∫do (remove espa√ßos extras e case-insensitive)
                normalized_content = robots_content.lower()
                
                # Procura por User-agent: * seguido de Disallow: /
                # Permite espa√ßos e quebras de linha entre as diretivas
                global_block_pattern = r'user-agent:\s*\*\s*.*?disallow:\s*/'
                
                if re.search(global_block_pattern, normalized_content, re.DOTALL):
                    result['indexable'] = False
                    result['issues'].append('üö® Robots.txt bloqueia o site inteiro (Disallow: /)')
                    print(f"  ‚ùå Bloqueio global detectado no robots.txt!")
                else:
                    # Verifica se tem Disallow: / sem User-agent espec√≠fico antes
                    lines = robots_content.split('\n')
                    user_agent_star = False
                    
                    for line in lines:
                        line_clean = line.strip().lower()
                        
                        if line_clean.startswith('user-agent:'):
                            if '*' in line_clean:
                                user_agent_star = True
                            else:
                                user_agent_star = False
                        
                        if user_agent_star and line_clean.startswith('disallow:'):
                            disallow_value = line_clean.split('disallow:')[1].strip()
                            if disallow_value == '/':
                                result['indexable'] = False
                                result['issues'].append('üö® Robots.txt bloqueia o site inteiro (Disallow: /)')
                                print(f"  ‚ùå Bloqueio global detectado no robots.txt!")
                                break
                    else:
                        print(f"  ‚úÖ Robots.txt n√£o bloqueia o site")
            else:
                print(f"  ‚ÑπÔ∏è Robots.txt n√£o encontrado (Status: {robots_response.status_code})")
        
        except httpx.HTTPError as e:
            print(f"  ‚ÑπÔ∏è Robots.txt n√£o acess√≠vel: {e}")
        
        # ============================================
        # RESULTADO FINAL
        # ============================================
        if result['indexable']:
            print(f"‚úÖ SEO Health Check: Site INDEX√ÅVEL")
        else:
            print(f"‚ùå SEO Health Check: Site BLOQUEADO - {len(result['issues'])} problemas encontrados")
        
        return result
    
    except Exception as e:
        print(f"‚ùå Erro inesperado no SEO Health Check para {domain}: {e}")
        result['error'] = str(e)
        result['indexable'] = True  # Assume OK em caso de erro (n√£o queremos falso positivo)
        return result


def check_wordpress_health(domain: str, timeout: int = DEFAULT_TIMEOUT) -> Dict[str, Any]:
    """
    Verifica se o site √© WordPress e realiza scan de seguran√ßa.
    
    Testes realizados:
    1. Detec√ß√£o de WordPress e vers√£o
    2. Arquivos sens√≠veis expostos
    3. User enumeration via API
    4. Debug log exposto
    
    Args:
        domain: Dom√≠nio a verificar (sem protocolo, ex: "example.com")
        timeout: Timeout para cada requisi√ß√£o (padr√£o: 5 segundos)
    
    Returns:
        Dict com:
        - is_wordpress: bool
        - wp_version: str ou None
        - vulnerabilities: List[Dict] com detalhes das vulnerabilidades
        - error: str ou None se houver erro
    
    Security Notes:
        - Usa User-Agent profissional para evitar bloqueios
        - Timeout curto para n√£o travar o worker
        - Verifica apenas arquivos comuns (n√£o √© invasivo)
        - N√£o tenta explorar vulnerabilidades, apenas detecta
    
    Example:
        >>> check_wordpress_health("wordpress-site.com")
        {
            'is_wordpress': True,
            'wp_version': '6.4.2',
            'vulnerabilities': [
                {'type': 'debug_log', 'file': '/wp-content/debug.log', 'severity': 'high'},
                {'type': 'user_enumeration', 'endpoint': '/wp-json/wp/v2/users', 'severity': 'medium'}
            ],
            'error': None
        }
    """
    # Remove protocolo e paths da URL
    clean_domain = re.sub(r'^https?://', '', domain)
    clean_domain = re.sub(r'^www\.', '', clean_domain)
    clean_domain = clean_domain.split('/')[0]
    
    # Tenta HTTPS primeiro, depois HTTP
    protocols = ['https', 'http']
    
    result = {
        'is_wordpress': False,
        'wp_version': None,
        'vulnerabilities': [],
        'error': None
    }
    
    # Headers profissionais para evitar bloqueios
    headers = {
        'User-Agent': 'SentinelWeb-SecurityScanner/1.0 (WordPress Health Check)',
        'Accept': 'text/html,application/json,*/*',
        'Accept-Language': 'en-US,en;q=0.9',
    }
    
    base_url = None
    
    # Tenta conectar com HTTPS, depois HTTP
    for protocol in protocols:
        try:
            test_url = f"{protocol}://{clean_domain}"
            response = requests.get(test_url, headers=headers, timeout=timeout, allow_redirects=True, verify=False)
            
            if response.status_code < 400:
                base_url = test_url
                break
        except Exception:
            continue
    
    if not base_url:
        result['error'] = "N√£o foi poss√≠vel conectar ao site"
        return result
    
    try:
        # ========================================
        # TESTE 1: Detec√ß√£o de WordPress e Vers√£o
        # ========================================
        
        # 1.1 - Verifica meta generator no HTML principal
        try:
            response = requests.get(base_url, headers=headers, timeout=timeout, allow_redirects=True, verify=False)
            html_content = response.text.lower()
            
            # Procura por indicadores de WordPress
            wp_indicators = [
                '/wp-content/',
                '/wp-includes/',
                'wordpress',
                'wp-json'
            ]
            
            has_wp_indicators = any(indicator in html_content for indicator in wp_indicators)
            
            # Procura vers√£o no meta generator
            version_match = re.search(r'<meta name="generator" content="wordpress\s+([0-9.]+)"', html_content)
            if version_match:
                result['is_wordpress'] = True
                result['wp_version'] = version_match.group(1)
                print(f"‚úÖ WordPress detectado via meta generator: vers√£o {result['wp_version']}")
            elif has_wp_indicators:
                result['is_wordpress'] = True
                print(f"‚úÖ WordPress detectado via indicadores no HTML")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao verificar HTML principal: {e}")
        
        # 1.2 - Tenta acessar readme.html
        if not result['wp_version']:
            try:
                readme_url = f"{base_url}/readme.html"
                response = requests.get(readme_url, headers=headers, timeout=timeout, verify=False)
                
                if response.status_code == 200:
                    result['is_wordpress'] = True
                    # Procura vers√£o no readme
                    version_match = re.search(r'Version\s+([0-9.]+)', response.text, re.IGNORECASE)
                    if version_match:
                        result['wp_version'] = version_match.group(1)
                        print(f"‚úÖ Vers√£o WordPress detectada via readme.html: {result['wp_version']}")
                    else:
                        print(f"‚úÖ WordPress detectado (readme.html acess√≠vel)")
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao verificar readme.html: {e}")
        
        # Se n√£o detectou WordPress, retorna
        if not result['is_wordpress']:
            print(f"‚ÑπÔ∏è  {clean_domain} n√£o parece ser WordPress")
            return result
        
        # ========================================
        # TESTE 2: Arquivos Sens√≠veis Expostos
        # ========================================
        
        sensitive_files = [
            {
                'path': '/wp-content/debug.log',
                'type': 'debug_log',
                'description': 'Debug log do WordPress exposto',
                'severity': 'high',
                'risk': 'Pode conter credenciais, paths do servidor e informa√ß√µes sens√≠veis'
            },
            {
                'path': '/wp-config.php.bak',
                'type': 'backup_config',
                'description': 'Backup do wp-config.php acess√≠vel',
                'severity': 'critical',
                'risk': 'Cont√©m credenciais do banco de dados'
            },
            {
                'path': '/wp-config.php.old',
                'type': 'backup_config',
                'description': 'Backup antigo do wp-config.php',
                'severity': 'critical',
                'risk': 'Cont√©m credenciais do banco de dados'
            },
            {
                'path': '/.git/config',
                'type': 'git_exposed',
                'description': 'Reposit√≥rio Git exposto',
                'severity': 'high',
                'risk': 'Pode expor c√≥digo-fonte e hist√≥rico de commits'
            },
            {
                'path': '/xmlrpc.php',
                'type': 'xmlrpc_enabled',
                'description': 'XML-RPC ativo (poss√≠vel vetor de ataque)',
                'severity': 'medium',
                'risk': 'Pode ser usado para brute force e DDoS'
            },
        ]
        
        for file_info in sensitive_files:
            try:
                file_url = f"{base_url}{file_info['path']}"
                response = requests.head(file_url, headers=headers, timeout=timeout, allow_redirects=False, verify=False)
                
                # Se n√£o tem HEAD, tenta GET
                if response.status_code == 405 or response.status_code == 404:
                    response = requests.get(file_url, headers=headers, timeout=timeout, allow_redirects=False, verify=False)
                
                if response.status_code == 200:
                    vulnerability = {
                        'type': file_info['type'],
                        'file': file_info['path'],
                        'description': file_info['description'],
                        'severity': file_info['severity'],
                        'risk': file_info['risk'],
                        'url': file_url
                    }
                    result['vulnerabilities'].append(vulnerability)
                    print(f"üö® Vulnerabilidade encontrada: {file_info['description']} ({file_url})")
                    
            except Exception as e:
                # Timeout ou erro de conex√£o √© esperado se o arquivo n√£o existe
                pass
        
        # ========================================
        # TESTE 3: User Enumeration via API
        # ========================================
        
        try:
            users_api_url = f"{base_url}/wp-json/wp/v2/users"
            response = requests.get(users_api_url, headers=headers, timeout=timeout, verify=False)
            
            if response.status_code == 200:
                try:
                    users_data = response.json()
                    
                    if isinstance(users_data, list) and len(users_data) > 0:
                        # Extrai usernames
                        usernames = []
                        for user in users_data[:5]:  # Limita a 5 usu√°rios
                            if isinstance(user, dict) and 'slug' in user:
                                usernames.append(user['slug'])
                        
                        vulnerability = {
                            'type': 'user_enumeration',
                            'endpoint': '/wp-json/wp/v2/users',
                            'description': 'Enumera√ß√£o de usu√°rios via REST API',
                            'severity': 'medium',
                            'risk': 'Exp√µe usernames que podem ser usados em ataques de brute force',
                            'users_found': len(users_data),
                            'sample_users': usernames,
                            'url': users_api_url
                        }
                        result['vulnerabilities'].append(vulnerability)
                        print(f"üö® User enumeration detectado: {len(users_data)} usu√°rios expostos")
                        
                except ValueError:
                    # N√£o √© JSON v√°lido
                    pass
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao verificar API de usu√°rios: {e}")
        
        # ========================================
        # TESTE 4: Directory Listing
        # ========================================
        
        try:
            uploads_url = f"{base_url}/wp-content/uploads/"
            response = requests.get(uploads_url, headers=headers, timeout=timeout, verify=False)
            
            if response.status_code == 200 and 'index of' in response.text.lower():
                vulnerability = {
                    'type': 'directory_listing',
                    'directory': '/wp-content/uploads/',
                    'description': 'Listagem de diret√≥rio habilitada',
                    'severity': 'low',
                    'risk': 'Permite navega√ß√£o nos arquivos do site',
                    'url': uploads_url
                }
                result['vulnerabilities'].append(vulnerability)
                print(f"‚ö†Ô∏è Directory listing habilitado em /wp-content/uploads/")
                
        except Exception as e:
            pass
        
        # ========================================
        # TESTE 5: Plugin CVE Scanner (OSV.dev)
        # ========================================
        
        plugins_with_cves = []
        
        try:
            # Extrai plugins do HTML
            plugins = extract_plugins_from_html(html_content)
            
            if plugins:
                # Verifica CVEs em paralelo usando asyncio
                import asyncio
                plugins_with_cves = asyncio.run(scan_plugins_vulnerabilities(plugins))
                
                # Adiciona vulnerabilidades de plugins ao resultado
                for plugin in plugins_with_cves:
                    if plugin['vulnerabilities']:
                        for cve in plugin['vulnerabilities']:
                            vulnerability = {
                                'type': 'plugin_cve',
                                'plugin_slug': plugin['slug'],
                                'plugin_version': plugin['version'],
                                'cve_id': cve['id'],
                                'description': f"CVE encontrado no plugin {plugin['slug']}",
                                'severity': cve['severity'].lower() if cve['severity'] != 'UNKNOWN' else 'medium',
                                'risk': cve['summary'],
                                'references': cve['references']
                            }
                            result['vulnerabilities'].append(vulnerability)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao verificar CVEs de plugins: {e}")
        
        # Resultado final
        vuln_count = len(result['vulnerabilities'])
        plugins_count = len(plugins_with_cves)
        
        if vuln_count > 0:
            print(f"üîç Scan WordPress conclu√≠do: {vuln_count} vulnerabilidade(s) encontrada(s)")
        else:
            print(f"‚úÖ Scan WordPress conclu√≠do: Nenhuma vulnerabilidade encontrada")
        
        if plugins_count > 0:
            print(f"üì¶ {plugins_count} plugin(s) detectado(s)")
        
        # Adiciona lista de plugins ao resultado
        result['plugins_detected'] = plugins_with_cves
        
        return result
        
    except Exception as e:
        result['error'] = f"Erro durante scan WordPress: {str(e)}"
        print(f"‚ùå Erro no WordPress scan para {clean_domain}: {e}")
        return result


def check_uptime(domain: str, timeout: int = DEFAULT_TIMEOUT, must_contain_keyword: Optional[str] = None) -> Tuple[bool, Optional[int], Optional[float], Optional[str]]:
    """
    Verifica se o site est√° online (HTTP 200) e opcionalmente se cont√©m uma palavra-chave.
    
    Como funciona:
    1. Faz uma requisi√ß√£o HTTP GET para o dom√≠nio
    2. Mede o tempo de resposta (lat√™ncia)
    3. Considera online se status code for 2xx ou 3xx
    4. Se must_contain_keyword for fornecida, verifica se existe no HTML
    
    Args:
        domain: O dom√≠nio a verificar (sem protocolo)
        timeout: Tempo m√°ximo de espera em segundos
        must_contain_keyword: Palavra-chave que deve existir no HTML (anti-defacement)
    
    Returns:
        Tuple[is_online, status_code, latency_ms, error_message]
    
    Security Note:
        - Usa timeout curto para evitar DoS na pr√≥pria aplica√ß√£o
        - Segue redirects (follow_redirects=True) para pegar status final
        - Verifica SSL por padr√£o (verify=True)
        - Verifica√ß√£o de keyword detecta poss√≠veis invas√µes/defacement
    """
    url = f"https://{domain}"
    error_message = None
    
    try:
        start_time = time.time()
        
        # Usa httpx para requisi√ß√µes HTTP modernas
        # verify=False temporariamente para sites com SSL inv√°lido n√£o falharem no uptime check
        with httpx.Client(timeout=timeout, follow_redirects=True, verify=False) as client:
            response = client.get(url)
        
        end_time = time.time()
        latency_ms = round((end_time - start_time) * 1000, 2)
        
        # Considera online se status for 2xx ou 3xx
        is_online = 200 <= response.status_code < 400
        
        # üîç VERIFICA√á√ÉO ANTI-DEFACEMENT
        if is_online and must_contain_keyword:
            keyword_found = must_contain_keyword.lower() in response.text.lower()
            
            if not keyword_found:
                # Site retornou 200, mas sem a palavra-chave esperada
                # Poss√≠vel invas√£o/defacement!
                is_online = False
                error_message = f"‚ö†Ô∏è POSS√çVEL INVAS√ÉO/DEFACEMENT: Palavra-chave '{must_contain_keyword}' n√£o encontrada no HTML"
                print(f"üö® ALERTA DE DEFACEMENT: {domain} - Keyword '{must_contain_keyword}' ausente!")
        
        return is_online, response.status_code, latency_ms, error_message
        
    except httpx.TimeoutException:
        return False, None, None, "Timeout na conex√£o"
    except httpx.ConnectError:
        # Tenta HTTP se HTTPS falhar
        try:
            url = f"http://{domain}"
            start_time = time.time()
            with httpx.Client(timeout=timeout, follow_redirects=True) as client:
                response = client.get(url)
            end_time = time.time()
            latency_ms = round((end_time - start_time) * 1000, 2)
            is_online = 200 <= response.status_code < 400
            
            # üîç VERIFICA√á√ÉO ANTI-DEFACEMENT (tamb√©m no HTTP)
            if is_online and must_contain_keyword:
                keyword_found = must_contain_keyword.lower() in response.text.lower()
                
                if not keyword_found:
                    is_online = False
                    error_message = f"‚ö†Ô∏è POSS√çVEL INVAS√ÉO/DEFACEMENT: Palavra-chave '{must_contain_keyword}' n√£o encontrada no HTML"
                    print(f"üö® ALERTA DE DEFACEMENT: {domain} - Keyword '{must_contain_keyword}' ausente!")
            
            return is_online, response.status_code, latency_ms, error_message
        except:
            return False, None, None, "Erro na conex√£o HTTP"
    except Exception as e:
        return False, None, None, f"Erro inesperado: {str(e)}"


def check_ssl_certificate(domain: str, timeout: int = DEFAULT_TIMEOUT) -> Dict[str, Any]:
    """
    Verifica a validade do certificado SSL/TLS.
    
    Como funciona:
    1. Estabelece conex√£o SSL com o servidor
    2. Obt√©m o certificado
    3. Verifica data de expira√ß√£o
    4. Extrai informa√ß√µes do emissor (CA)
    
    Args:
        domain: O dom√≠nio a verificar
        timeout: Tempo m√°ximo de espera
    
    Returns:
        Dict com: valid, days_remaining, issuer, error
    
    Security Notes:
        - Certificados expirados ou inv√°lidos s√£o riscos de seguran√ßa
        - Alerta se faltar menos de 30 dias para expirar
        - Verifica apenas o certificado, n√£o a cadeia completa (MVP)
    """
    result = {
        "valid": None,
        "days_remaining": None,
        "issuer": None,
        "error": None
    }
    
    try:
        # Cria contexto SSL que aceita qualquer certificado (para inspe√ß√£o)
        context = ssl.create_default_context()
        
        # Conecta ao servidor na porta 443 (HTTPS)
        with socket.create_connection((domain, 443), timeout=timeout) as sock:
            with context.wrap_socket(sock, server_hostname=domain) as ssock:
                # Obt√©m o certificado em formato bin√°rio (DER)
                cert_der = ssock.getpeercert(binary_form=True)
                
                # Converte para objeto X509 para an√°lise
                cert = crypto.load_certificate(crypto.FILETYPE_ASN1, cert_der)
                
                # Data de expira√ß√£o
                not_after = cert.get_notAfter().decode('utf-8')
                # Formato: YYYYMMDDhhmmssZ
                expiry_date = datetime.strptime(not_after, '%Y%m%d%H%M%SZ')
                
                # Calcula dias restantes
                days_remaining = (expiry_date - datetime.utcnow()).days
                
                # Extrai informa√ß√µes do emissor (CA)
                issuer = cert.get_issuer()
                issuer_str = issuer.CN if issuer.CN else str(issuer)
                
                result["valid"] = days_remaining > 0
                result["days_remaining"] = days_remaining
                result["issuer"] = issuer_str
                
    except ssl.SSLCertVerificationError as e:
        result["valid"] = False
        result["error"] = f"Certificado inv√°lido: {str(e)}"
    except socket.timeout:
        result["error"] = "Timeout ao verificar SSL"
    except socket.gaierror:
        result["error"] = "N√£o foi poss√≠vel resolver o dom√≠nio"
    except ConnectionRefusedError:
        result["error"] = "Conex√£o recusada na porta 443"
    except Exception as e:
        result["error"] = f"Erro ao verificar SSL: {str(e)}"
    
    return result


def check_port(host: str, port: int, timeout: int = DEFAULT_TIMEOUT) -> bool:
    """
    Verifica se uma porta espec√≠fica est√° aberta.
    
    Como funciona:
    1. Tenta estabelecer conex√£o TCP na porta
    2. Se conectar, a porta est√° aberta
    3. Se timeout ou recusar, a porta est√° fechada
    
    Args:
        host: O dom√≠nio ou IP
        port: N√∫mero da porta
        timeout: Tempo m√°ximo de espera
    
    Returns:
        True se a porta est√° aberta, False caso contr√°rio
    
    Security Notes:
        - Portas abertas de bancos de dados s√£o CR√çTICAS
        - FTP (21) e Telnet (23) s√£o inseguros por natureza
        - SSH (22) pode ser leg√≠timo, mas deve ser monitorado
    """
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(timeout)
        result = sock.connect_ex((host, port))
        sock.close()
        
        # connect_ex retorna 0 se a conex√£o foi bem sucedida
        return result == 0
        
    except socket.gaierror:
        # N√£o conseguiu resolver o hostname
        return False
    except socket.timeout:
        return False
    except Exception:
        return False


def scan_critical_ports(domain: str, timeout: int = 2) -> List[int]:
    """
    Escaneia portas cr√≠ticas que podem representar riscos de seguran√ßa.
    
    Args:
        domain: O dom√≠nio a escanear
        timeout: Tempo m√°ximo por porta (menor para n√£o demorar)
    
    Returns:
        Lista de portas abertas encontradas
    
    Security Notes:
        - Este scan √© b√°sico e n√£o substitui ferramentas como nmap
        - Portas abertas n√£o significam necessariamente vulnerabilidade
        - Mas exposi√ß√£o desnecess√°ria aumenta superf√≠cie de ataque
    """
    open_ports = []
    
    # Resolve o dom√≠nio para IP primeiro
    try:
        ip = socket.gethostbyname(domain)
    except socket.gaierror:
        return open_ports  # Retorna vazio se n√£o resolver
    
    # Verifica cada porta cr√≠tica
    for port in CRITICAL_PORTS.keys():
        if check_port(ip, port, timeout):
            open_ports.append(port)
    
    return open_ports


def full_scan(domain: str, must_contain_keyword: Optional[str] = None) -> ScanResult:
    """
    Executa uma verifica√ß√£o completa do dom√≠nio.
    
    Esta √© a fun√ß√£o principal chamada pelo Celery worker.
    Executa todos os checks em sequ√™ncia:
    1. Uptime Check (HTTP) + Verifica√ß√£o Anti-Defacement
    2. SSL Check (Certificado)
    3. Port Scan (Portas Cr√≠ticas)
    
    Args:
        domain: O dom√≠nio a verificar
        must_contain_keyword: Palavra-chave que deve existir no HTML (anti-defacement)
    
    Returns:
        ScanResult com todos os dados coletados
    
    Note:
        Usa try/except em cada check para garantir que
        uma falha em um n√£o afete os outros.
    """
    result = ScanResult()
    
    # 1. Verifica Uptime + Anti-Defacement
    try:
        is_online, status_code, latency, error_msg = check_uptime(domain, must_contain_keyword=must_contain_keyword)
        result.is_online = is_online
        result.http_status_code = status_code
        result.latency_ms = latency
        if error_msg:
            result.error_message = error_msg
    except Exception as e:
        result.is_online = False
        result.error_message = f"Erro no check de uptime: {str(e)}"
    
    # 2. Verifica SSL
    try:
        ssl_result = check_ssl_certificate(domain)
        result.ssl_valid = ssl_result["valid"]
        result.ssl_days_remaining = ssl_result["days_remaining"]
        result.ssl_issuer = ssl_result["issuer"]
        result.ssl_error = ssl_result["error"]
    except Exception as e:
        result.ssl_error = f"Erro no check de SSL: {str(e)}"
    
    # 3. Scan de Portas
    try:
        result.open_ports = scan_critical_ports(domain)
    except Exception as e:
        # N√£o deixa falha no port scan quebrar todo o resultado
        pass
    
    return result


# Vers√£o ass√≠ncrona para uso futuro
async def async_full_scan(domain: str) -> ScanResult:
    """
    Vers√£o ass√≠ncrona do full_scan.
    √ötil para quando migrarmos para workers ass√≠ncronos.
    """
    # Por enquanto, apenas wrapper s√≠ncrono
    return await asyncio.get_event_loop().run_in_executor(
        None, full_scan, domain
    )


def check_pagespeed(url: str, strategy: str = "mobile", timeout: float = 30.0) -> Dict[str, Any]:
    """
    Verifica a performance de um site usando Google PageSpeed Insights API v5.
    
    Args:
        url: URL completa do site (ex: https://exemplo.com)
        strategy: 'mobile' ou 'desktop'
        timeout: Timeout da requisi√ß√£o (API do Google pode demorar 10-30s)
    
    Returns:
        Dict com scores e m√©tricas:
        {
            "success": bool,
            "performance_score": int (0-100),
            "seo_score": int (0-100),
            "accessibility_score": int (0-100),
            "best_practices_score": int (0-100),
            "metrics": {
                "first_contentful_paint": float (segundos),
                "largest_contentful_paint": float (segundos),
                "cumulative_layout_shift": float,
                "speed_index": float (segundos),
                "total_blocking_time": float (ms)
            },
            "error": str (se houver)
        }
    
    Note:
        - Requer GOOGLE_PAGESPEED_API_KEY no .env
        - Quota gratuita: 25,000 requisi√ß√µes/dia
        - Recomendado: rodar apenas 1x por dia por site
    
    Exemplo:
        result = check_pagespeed("https://exemplo.com", strategy="mobile")
        if result["success"]:
            print(f"Performance Score: {result['performance_score']}/100")
    """
    
    # Pega API Key do ambiente
    api_key = os.getenv("GOOGLE_PAGESPEED_API_KEY", "")
    
    if not api_key:
        return {
            "success": False,
            "error": "GOOGLE_PAGESPEED_API_KEY n√£o configurada no .env",
            "performance_score": None,
            "seo_score": None,
            "accessibility_score": None,
            "best_practices_score": None,
            "metrics": {}
        }
    
    # Garante que a URL tem protocolo
    if not url.startswith(("http://", "https://")):
        url = f"https://{url}"
    
    # API Endpoint
    api_url = "https://www.googleapis.com/pagespeedonline/v5/runPagespeed"
    
    # Par√¢metros da requisi√ß√£o
    params = {
        "url": url,
        "strategy": strategy,  # mobile ou desktop
        "key": api_key,
        "category": ["performance", "seo", "accessibility", "best-practices"]  # Todas as categorias
    }
    
    try:
        print(f"üöÄ Iniciando PageSpeed Insights para {url} ({strategy})...")
        
        response = requests.get(
            api_url,
            params=params,
            timeout=timeout
        )
        
        response.raise_for_status()
        data = response.json()
        
        # Extrai os scores (v√™m de 0 a 1, convertemos para 0-100)
        lighthouse = data.get("lighthouseResult", {})
        categories = lighthouse.get("categories", {})
        
        performance_score = None
        seo_score = None
        accessibility_score = None
        best_practices_score = None
        
        if "performance" in categories:
            performance_score = int(categories["performance"]["score"] * 100)
        
        if "seo" in categories:
            seo_score = int(categories["seo"]["score"] * 100)
        
        if "accessibility" in categories:
            accessibility_score = int(categories["accessibility"]["score"] * 100)
        
        if "best-practices" in categories:
            best_practices_score = int(categories["best-practices"]["score"] * 100)
        
        # Extrai m√©tricas Core Web Vitals
        audits = lighthouse.get("audits", {})
        metrics = {}
        
        # First Contentful Paint (FCP)
        if "first-contentful-paint" in audits:
            fcp = audits["first-contentful-paint"].get("numericValue", 0)
            metrics["first_contentful_paint"] = round(fcp / 1000, 2)  # ms para segundos
        
        # Largest Contentful Paint (LCP)
        if "largest-contentful-paint" in audits:
            lcp = audits["largest-contentful-paint"].get("numericValue", 0)
            metrics["largest_contentful_paint"] = round(lcp / 1000, 2)
        
        # Cumulative Layout Shift (CLS)
        if "cumulative-layout-shift" in audits:
            cls = audits["cumulative-layout-shift"].get("numericValue", 0)
            metrics["cumulative_layout_shift"] = round(cls, 3)
        
        # Speed Index
        if "speed-index" in audits:
            si = audits["speed-index"].get("numericValue", 0)
            metrics["speed_index"] = round(si / 1000, 2)
        
        # Total Blocking Time (TBT)
        if "total-blocking-time" in audits:
            tbt = audits["total-blocking-time"].get("numericValue", 0)
            metrics["total_blocking_time"] = round(tbt, 0)  # j√° est√° em ms
        
        print(f"‚úÖ PageSpeed conclu√≠do - Performance: {performance_score}/100")
        
        return {
            "success": True,
            "performance_score": performance_score,
            "seo_score": seo_score,
            "accessibility_score": accessibility_score,
            "best_practices_score": best_practices_score,
            "metrics": metrics,
            "error": None
        }
    
    except requests.exceptions.Timeout:
        return {
            "success": False,
            "error": "Timeout: A API do Google demorou mais de 30 segundos para responder",
            "performance_score": None,
            "seo_score": None,
            "accessibility_score": None,
            "best_practices_score": None,
            "metrics": {}
        }
    
    except requests.exceptions.RequestException as e:
        return {
            "success": False,
            "error": f"Erro na requisi√ß√£o: {str(e)}",
            "performance_score": None,
            "seo_score": None,
            "accessibility_score": None,
            "best_practices_score": None,
            "metrics": {}
        }
    
    except (KeyError, ValueError, TypeError) as e:
        return {
            "success": False,
            "error": f"Erro ao processar resposta da API: {str(e)}",
            "performance_score": None,
            "seo_score": None,
            "accessibility_score": None,
            "best_practices_score": None,
            "metrics": {}
        }


# ============================================
# VISUAL REGRESSION TESTING
# ============================================

async def take_screenshot(url: str, site_id: int, screenshot_type: str = "current") -> Optional[str]:
    """
    Captura um screenshot de uma URL usando Playwright.
    
    Args:
        url: URL completa do site (com http:// ou https://)
        site_id: ID do site no banco de dados
        screenshot_type: Tipo do screenshot ("current", "baseline")
    
    Returns:
        Caminho do arquivo salvo ou None em caso de erro
    
    Performance Notes:
        - Usa chromium headless para menor overhead
        - Timeout de 30s para evitar travar o worker
        - Aguarda networkidle para garantir conte√∫do completo
    """
    from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout
    
    try:
        # Garante que o diret√≥rio existe
        screenshots_dir = "static/screenshots"
        os.makedirs(screenshots_dir, exist_ok=True)
        
        # Define o nome do arquivo
        filename = f"{site_id}_{screenshot_type}.png"
        filepath = os.path.join(screenshots_dir, filename)
        
        # Normaliza a URL
        if not url.startswith(('http://', 'https://')):
            url = f"https://{url}"
        
        async with async_playwright() as p:
            # Lan√ßa o browser (chromium √© mais leve que firefox/webkit)
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-accelerated-2d-canvas',
                    '--disable-gpu'
                ]
            )
            
            # Cria um contexto com viewport padr√£o (desktop)
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            
            page = await context.new_page()
            
            # Navega at√© a p√°gina com timeout de 30s
            await page.goto(url, wait_until='networkidle', timeout=30000)
            
            # Aguarda 2s extras para garantir que tudo carregou (JS, imagens lazy)
            await page.wait_for_timeout(2000)
            
            # Tira o screenshot em fullpage
            await page.screenshot(path=filepath, full_page=True)
            
            await browser.close()
            
            return filepath
            
    except PlaywrightTimeout:
        print(f"‚è±Ô∏è  Timeout ao acessar {url} para screenshot")
        return None
        
    except Exception as e:
        print(f"‚ùå Erro ao tirar screenshot de {url}: {str(e)}")
        return None


def compare_images(img1_path: str, img2_path: str) -> float:
    """
    Compara duas imagens e retorna a porcentagem de diferen√ßa.
    
    Args:
        img1_path: Caminho da imagem 1 (baseline)
        img2_path: Caminho da imagem 2 (current)
    
    Returns:
        Float com a porcentagem de diferen√ßa (0.0 - 100.0)
        
    Algorithm:
        1. Carrega as imagens com Pillow
        2. Redimensiona para o mesmo tamanho (usa o menor)
        3. Converte para arrays numpy
        4. Calcula a diferen√ßa absoluta pixel por pixel
        5. Retorna a m√©dia como porcentagem
        
    Performance:
        - Usa numpy para opera√ß√µes vetorizadas (muito r√°pido)
        - Redimensiona imagens grandes para evitar overhead
    """
    from PIL import Image
    import numpy as np
    
    try:
        # Verifica se os arquivos existem
        if not os.path.exists(img1_path) or not os.path.exists(img2_path):
            print(f"‚ö†Ô∏è  Arquivo n√£o encontrado para compara√ß√£o")
            return 0.0
        
        # Carrega as imagens
        img1 = Image.open(img1_path).convert('RGB')
        img2 = Image.open(img2_path).convert('RGB')
        
        # Se tamanhos forem diferentes, redimensiona para o menor
        if img1.size != img2.size:
            # Pega as dimens√µes m√≠nimas
            min_width = min(img1.width, img2.width)
            min_height = min(img1.height, img2.height)
            
            img1 = img1.resize((min_width, min_height), Image.Resampling.LANCZOS)
            img2 = img2.resize((min_width, min_height), Image.Resampling.LANCZOS)
        
        # Converte para arrays numpy
        arr1 = np.array(img1, dtype=np.float64)
        arr2 = np.array(img2, dtype=np.float64)
        
        # Calcula a diferen√ßa absoluta
        diff = np.abs(arr1 - arr2)
        
        # Calcula a m√©dia da diferen√ßa (0-255 por canal RGB)
        mean_diff = np.mean(diff)
        
        # Converte para porcentagem (255 = 100%)
        percent_diff = (mean_diff / 255.0) * 100.0
        
        return round(percent_diff, 2)
        
    except Exception as e:
        print(f"‚ùå Erro ao comparar imagens: {str(e)}")
        return 0.0


def create_diff_image(img1_path: str, img2_path: str, output_path: str) -> bool:
    """
    Cria uma imagem de diferen√ßa visual (√∫til para debug).
    
    Args:
        img1_path: Imagem baseline
        img2_path: Imagem current
        output_path: Onde salvar a imagem de diferen√ßa
        
    Returns:
        True se sucesso, False se erro
    """
    from PIL import Image
    import numpy as np
    
    try:
        # Carrega as imagens
        img1 = Image.open(img1_path).convert('RGB')
        img2 = Image.open(img2_path).convert('RGB')
        
        # Redimensiona se necess√°rio
        if img1.size != img2.size:
            min_width = min(img1.width, img2.width)
            min_height = min(img1.height, img2.height)
            img1 = img1.resize((min_width, min_height), Image.Resampling.LANCZOS)
            img2 = img2.resize((min_width, min_height), Image.Resampling.LANCZOS)
        
        # Converte para arrays
        arr1 = np.array(img1, dtype=np.float64)
        arr2 = np.array(img2, dtype=np.float64)
        
        # Calcula a diferen√ßa e amplifica para visualiza√ß√£o
        diff = np.abs(arr1 - arr2) * 3  # Multiplica por 3 para destacar diferen√ßas
        diff = np.clip(diff, 0, 255)  # Garante que fica no range 0-255
        
        # Converte de volta para imagem
        diff_img = Image.fromarray(diff.astype(np.uint8))
        
        # Salva
        diff_img.save(output_path)
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro ao criar imagem de diferen√ßa: {str(e)}")
        return False


# ============================================
# PLUGIN CVE SCANNER (OSV.dev Integration)
# ============================================

def extract_plugins_from_html(html_content: str) -> List[Dict[str, str]]:
    """
    Extrai plugins WordPress do HTML usando regex.
    
    Procura por padr√µes como:
    - /wp-content/plugins/plugin-name/assets/style.css?ver=1.2.3
    - /wp-content/plugins/plugin-name/js/script.js?ver=2.0.1
    
    Args:
        html_content: Conte√∫do HTML da p√°gina
    
    Returns:
        Lista de dicion√°rios: [{'slug': 'nome-plugin', 'version': '1.2.3'}]
    
    Example:
        >>> html = '<link href="/wp-content/plugins/contact-form-7/includes/css/styles.css?ver=5.9.8" />'
        >>> extract_plugins_from_html(html)
        [{'slug': 'contact-form-7', 'version': '5.9.8'}]
    """
    plugins = {}
    
    # Regex para encontrar plugins com vers√£o
    # Padr√£o: /wp-content/plugins/PLUGIN-NAME/...?ver=VERSION
    pattern = r'/wp-content/plugins/([a-z0-9\-_]+)/[^"\']*\?ver=([0-9\.]+)'
    
    matches = re.finditer(pattern, html_content, re.IGNORECASE)
    
    for match in matches:
        slug = match.group(1)
        version = match.group(2)
        
        # Guarda apenas a vers√£o mais alta de cada plugin
        if slug not in plugins or version > plugins[slug]:
            plugins[slug] = version
    
    # Converte para lista de dicion√°rios
    result = [
        {'slug': slug, 'version': version}
        for slug, version in plugins.items()
    ]
    
    print(f"üîç Plugins detectados: {len(result)}")
    for plugin in result:
        print(f"   - {plugin['slug']} v{plugin['version']}")
    
    return result


async def check_cves_osv_async(slug: str, version: str) -> List[Dict[str, Any]]:
    """
    Consulta a API do OSV.dev para verificar CVEs de um plugin.
    
    Args:
        slug: Nome do plugin (ex: 'contact-form-7')
        version: Vers√£o do plugin (ex: '5.9.8')
    
    Returns:
        Lista de vulnerabilidades encontradas:
        [
            {
                'id': 'CVE-2023-1234',
                'summary': 'SQL Injection vulnerability',
                'severity': 'HIGH',
                'references': ['https://...']
            }
        ]
    """
    url = "https://api.osv.dev/v1/query"
    
    # Formato do package name para WordPress plugins no OSV.dev
    payload = {
        "package": {
            "name": slug,  # OSV.dev usa o slug direto para WordPress plugins
            "ecosystem": "WordPress"
        },
        "version": version
    }
    
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.post(url, json=payload)
            
            if response.status_code != 200:
                return []
            
            data = response.json()
            
            # Se n√£o h√° vulnerabilidades, a resposta vem vazia
            if "vulns" not in data or not data["vulns"]:
                return []
            
            vulnerabilities = []
            
            for vuln in data["vulns"]:
                # Extrai severidade (pode n√£o estar presente)
                severity = "UNKNOWN"
                if "severity" in vuln:
                    if isinstance(vuln["severity"], list) and len(vuln["severity"]) > 0:
                        severity = vuln["severity"][0].get("type", "UNKNOWN")
                
                # Extrai refer√™ncias (links para mais informa√ß√µes)
                references = []
                if "references" in vuln:
                    references = [ref.get("url", "") for ref in vuln["references"] if "url" in ref]
                
                vulnerabilities.append({
                    "id": vuln.get("id", "UNKNOWN"),
                    "summary": vuln.get("summary", "No description available"),
                    "severity": severity,
                    "references": references[:3]  # Limita a 3 refer√™ncias
                })
            
            return vulnerabilities
            
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao consultar OSV.dev para {slug}@{version}: {e}")
        return []


async def scan_plugins_vulnerabilities(plugins: List[Dict[str, str]]) -> List[Dict[str, Any]]:
    """
    Verifica vulnerabilidades de m√∫ltiplos plugins em paralelo.
    
    Args:
        plugins: Lista de plugins [{'slug': 'nome', 'version': '1.0'}]
    
    Returns:
        Lista de plugins com vulnerabilidades:
        [
            {
                'slug': 'contact-form-7',
                'version': '5.9.8',
                'vulnerabilities': [...]
            }
        ]
    """
    if not plugins:
        return []
    
    print(f"üîí Verificando vulnerabilidades de {len(plugins)} plugins...")
    
    # Cria tasks para verificar todos os plugins em paralelo
    tasks = [
        check_cves_osv_async(plugin['slug'], plugin['version'])
        for plugin in plugins
    ]
    
    # Executa todas as consultas em paralelo
    results = await asyncio.gather(*tasks)
    
    # Combina plugins com seus CVEs
    plugins_with_cves = []
    for i, plugin in enumerate(plugins):
        vulnerabilities = results[i]
        
        plugin_data = {
            'slug': plugin['slug'],
            'version': plugin['version'],
            'vulnerabilities': vulnerabilities
        }
        
        plugins_with_cves.append(plugin_data)
        
        if vulnerabilities:
            print(f"   üö® {plugin['slug']} v{plugin['version']}: {len(vulnerabilities)} vulnerabilidade(s) encontrada(s)")
        else:
            print(f"   ‚úÖ {plugin['slug']} v{plugin['version']}: Nenhuma vulnerabilidade conhecida")
    
    return plugins_with_cves


def audit_security_headers(headers: dict) -> Dict[str, Any]:
    """
    Audita headers de seguran√ßa HTTP e d√° uma nota.
    
    Args:
        headers: Dict com headers HTTP da resposta
    
    Returns:
        {
            'grade': 'A' | 'B' | 'C' | 'F',
            'score': 100,
            'headers_found': [...],
            'headers_missing': [...],
            'recommendations': [...]
        }
    """
    critical_headers = {
        'strict-transport-security': 'HSTS - For√ßa HTTPS',
        'content-security-policy': 'CSP - Previne XSS',
        'x-frame-options': 'Previne Clickjacking',
        'x-content-type-options': 'Previne MIME Sniffing',
        'referrer-policy': 'Controla informa√ß√µes de refer√™ncia',
        'permissions-policy': 'Controla permiss√µes de recursos'
    }
    
    headers_lower = {k.lower(): v for k, v in headers.items()}
    found = []
    missing = []
    
    for header, description in critical_headers.items():
        if header in headers_lower:
            found.append({
                'header': header,
                'value': headers_lower[header],
                'description': description
            })
        else:
            missing.append({
                'header': header,
                'description': description
            })
    
    # Calcula nota baseada nos headers cr√≠ticos principais (os 4 primeiros)
    critical_count = sum(1 for h in found if h['header'] in list(critical_headers.keys())[:4])
    score = (critical_count / 4) * 100
    
    if score == 100:
        grade = 'A'
    elif score >= 75:
        grade = 'B'
    elif score >= 50:
        grade = 'C'
    else:
        grade = 'F'
    
    return {
        'grade': grade,
        'score': int(score),
        'headers_found': found,
        'headers_missing': missing,
        'recommendations': [f"Adicione header: {h['header']}" for h in missing]
    }


def detect_tech_stack(url: str, timeout: int = DEFAULT_TIMEOUT) -> Dict[str, Any]:
    """
    Detecta tecnologias e VERS√ïES usando Wappalyzer.
    
    Args:
        url: URL completa do site (ex: https://example.com)
        timeout: Timeout em segundos
    
    Returns:
        {
            'technologies': [
                {'name': 'Nginx', 'version': '1.18.0', 'categories': ['Web Servers']},
                {'name': 'React', 'version': None, 'categories': ['JavaScript Frameworks']}
            ],
            'detected_at': datetime
        }
    """
    try:
        from Wappalyzer import Wappalyzer, WebPage
        
        wappalyzer = Wappalyzer.latest()
        webpage = WebPage.new_from_url(url, timeout=timeout)
        technologies = wappalyzer.analyze_with_versions(webpage)
        
        results = []
        for tech_name, tech_info in technologies.items():
            # Wappalyzer retorna dict com 'versions' (lista) e 'categories' (lista)
            versions = tech_info.get('versions', [])
            version = versions[0] if versions else None
            
            results.append({
                'name': tech_name,
                'version': version,
                'categories': tech_info.get('categories', []),
                'version_detected': bool(version)
            })
        
        return {
            'technologies': results,
            'detected_at': datetime.now().isoformat(),
            'success': True
        }
        
    except Exception as e:
        print(f"‚ùå Erro ao detectar tech stack: {e}")
        return {
            'technologies': [],
            'detected_at': datetime.now().isoformat(),
            'success': False,
            'error': str(e)
        }


def map_category_to_ecosystem(categories: List[str]) -> str:
    """
    Mapeia categorias de tecnologia para ecosystems do OSV.dev.
    
    Args:
        categories: Lista de categorias do Wappalyzer
    
    Returns:
        Ecosystem string: 'npm', 'PyPI', 'Go', etc.
    """
    category_mapping = {
        'JavaScript frameworks': 'npm',
        'JavaScript libraries': 'npm',
        'UI frameworks': 'npm',
        'Node.js': 'npm',
        'Programming languages': 'PyPI',  # Assumindo Python se n√£o especificado
        'Web frameworks': 'PyPI',
        'Databases': 'Maven',  # Muitos DBs usam Java
    }
    
    for category in categories:
        if category in category_mapping:
            return category_mapping[category]
    
    # Default para npm (JavaScript √© o mais comum na web)
    return 'npm'


def query_osv_vulnerabilities(package_name: str, version: str, ecosystem: str = 'npm') -> List[Dict]:
    """
    Consulta OSV.dev API para CVEs de um pacote espec√≠fico.
    
    Args:
        package_name: Nome do pacote (ex: 'react', 'nginx')
        version: Vers√£o exata (ex: '16.8.0')
        ecosystem: npm, PyPI, Go, Maven, etc.
    
    Returns:
        Lista de vulnerabilidades encontradas
    """
    if not version or version == 'None':
        return []  # Sem vers√£o, n√£o conseguimos verificar
    
    api_url = "https://api.osv.dev/v1/query"
    payload = {
        "version": version,
        "package": {
            "name": package_name.lower(),
            "ecosystem": ecosystem
        }
    }
    
    try:
        response = requests.post(api_url, json=payload, timeout=10)
        if response.status_code == 200:
            data = response.json()
            vulns = data.get('vulns', [])
            
            # Formata resultados
            results = []
            for v in vulns:
                # Extrai severity
                severity_list = v.get('severity', [])
                severity = 'UNKNOWN'
                if severity_list:
                    severity = severity_list[0].get('score', 'UNKNOWN')
                
                results.append({
                    'cve_id': v.get('id', 'N/A'),
                    'summary': v.get('summary', 'No summary available')[:200],  # Limita tamanho
                    'severity': severity,
                    'published': v.get('published', 'N/A'),
                    'modified': v.get('modified', 'N/A')
                })
            
            return results
            
        else:
            print(f"‚ö†Ô∏è  OSV.dev retornou status {response.status_code}")
            return []
            
    except Exception as e:
        print(f"‚ùå Erro ao consultar OSV.dev: {e}")
        return []


def check_general_security(url: str, timeout: int = DEFAULT_TIMEOUT) -> Dict[str, Any]:
    """
    Fun√ß√£o orquestradora: Detecta tech stack, consulta CVEs e audita headers.
    
    Args:
        url: URL completa do site
        timeout: Timeout em segundos
    
    Returns:
        {
            'tech_stack': {...},
            'vulnerabilities': [...],
            'security_headers': {...},
            'timestamp': datetime,
            'errors': [...]
        }
    """
    results = {
        'tech_stack': None,
        'vulnerabilities': [],
        'security_headers': None,
        'timestamp': datetime.now().isoformat(),
        'errors': []
    }
    
    try:
        # 1. Faz request para pegar headers
        print(f"üîç Fazendo request para {url}...")
        response = httpx.get(url, timeout=timeout, follow_redirects=True)
        
        # 2. Audita headers de seguran√ßa (sempre funciona)
        print(f"üîê Auditando headers de seguran√ßa...")
        results['security_headers'] = audit_security_headers(dict(response.headers))
        
        # 3. Detecta tecnologias
        print(f"üõ†Ô∏è  Detectando tecnologias...")
        tech_result = detect_tech_stack(url, timeout)
        results['tech_stack'] = tech_result
        
        # 4. Para cada tecnologia com vers√£o, busca CVEs (com rate limiting)
        if tech_result.get('success') and tech_result.get('technologies'):
            print(f"üîé Buscando vulnerabilidades...")
            for tech in tech_result['technologies']:
                if tech.get('version'):
                    ecosystem = map_category_to_ecosystem(tech.get('categories', []))
                    print(f"   ‚Ä¢ {tech['name']} v{tech['version']} ({ecosystem})...")
                    
                    vulns = query_osv_vulnerabilities(
                        package_name=tech['name'].lower(),
                        version=tech['version'],
                        ecosystem=ecosystem
                    )
                    
                    if vulns:
                        print(f"     ‚ö†Ô∏è  {len(vulns)} vulnerabilidade(s) encontrada(s)")
                        for vuln in vulns:
                            vuln['technology'] = tech['name']
                            vuln['version'] = tech['version']
                            results['vulnerabilities'].append(vuln)
                    
                    # Rate limiting: espera 1 segundo entre requests ao OSV.dev
                    time.sleep(1)
        
        print(f"‚úÖ Scan conclu√≠do!")
        
    except Exception as e:
        error_msg = f"Erro em check_general_security: {str(e)}"
        results['errors'].append(error_msg)
        print(f"‚ùå {error_msg}")
    
    return results


if __name__ == "__main__":
    # Teste manual do scanner
    import sys
    
    if len(sys.argv) > 1:
        test_domain = sys.argv[1]
    else:
        test_domain = "google.com"
    
    print(f"üîç Escaneando {test_domain}...")
    result = full_scan(test_domain)
    
    print(f"\nüìä Resultados:")
    print(f"   Online: {'‚úÖ' if result.is_online else '‚ùå'}")
    print(f"   Status HTTP: {result.http_status_code}")
    print(f"   Lat√™ncia: {result.latency_ms}ms")
    print(f"   SSL V√°lido: {'‚úÖ' if result.ssl_valid else '‚ùå'}")
    print(f"   Dias para SSL expirar: {result.ssl_days_remaining}")
    print(f"   Emissor SSL: {result.ssl_issuer}")
    print(f"   Portas Abertas: {result.open_ports if result.open_ports else 'Nenhuma'}")
